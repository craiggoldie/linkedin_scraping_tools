{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Web-Scraping Toolkit\n",
    "\n",
    "This code asks for your LinkedIn credentials, the city/region where you are interested in working, and the keywords in titles/taglines for all people you want to 'view'. It will then:\n",
    "\n",
    "1. look at all* current job postings in your city of interest and save the posting organizations/companies to a csv file (in the same directory as this notebook)\n",
    "2. use that list of companies and search its people for roles that match your keywords (e.g. \"Purchasing Manager\")\n",
    "3. save the names, jobtitles, locations, and linkedin urls for those people in an Excel file\n",
    "4. view the profiles of any people from (3) that are located in your city of interest\n",
    "\n",
    "The idea is that you will be able to obtain a list of companies and/or people to attempt to network with, and you will have viewed the profiles of these people--which could lead to you getting more profile views from relevant people that may be considering hiring someone like you.\n",
    "\n",
    "\n",
    "Prior to running this program, make sure:\n",
    "1. chrome is installed (see [chrome website](https://www.google.co.uk/chrome/?brand=CHBD&gclid=Cj0KCQiA2o_fBRC8ARIsAIOyQ-mBWe_td_tlfyeh_TWRbyDCe7zo6R65xYhObb42egIYBfkRnlW4_MUaAtdvEALw_wcB&gclsrc=aw.ds))\n",
    "2. chromedriver.exe file is in the same folder as this file (download the Python one at [SeleniumHQ](https://www.seleniumhq.org/download/))\n",
    "3. python and all the required libraries (pandas, os, selenium, time, random, re) are installed on your machine\n",
    "5. search_terms.csv is saved in the same folder as this file with search terms listed in column A from cell A1\n",
    "\n",
    "*Note that LinkedIn limits the job postings to the first 40 pages of results--ranked by how relevant LinkedIn thinks the jobs are to you, based on previous use of their website, your profile experience, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libraries and inputs\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import time\n",
    "#os.chdir('C:\\\\Users\\\\craig\\\\OneDrive - Dufrain Consulting\\\\analytics_craig\\\\tasks\\\\201804\\\\linkedin_scraping')\n",
    "import random\n",
    "import re\n",
    "#import linkedin_creds #this needs phased out\n",
    "\n",
    "#Switch jupyter to enable multiple outputs per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Change Jupyter cell width to 100% of browser\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style> .container { width:100% !important; }</style>\"))\n",
    "#display(HTML('<style> div.prompt {display:none} </style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for scrolling down a page (that will be used at least twice in this notebook)\n",
    "def scrolljs():\n",
    "    pg_height=driver.execute_script('return document.body.scrollHeight') #get height of page\n",
    "    scroll_step=pg_height/5 #define size of scroll increments\n",
    "    for i in range(1,6): #initiate the loop\n",
    "        #print(i) #print current iteration (for debugging only)\n",
    "        driver.execute_script('window.scrollTo(0,'+str(scroll_step*i)+');') #scroll another 1/5th down\n",
    "        time.sleep(.5) #give the js a chance to load html content\n",
    "    driver.execute_script('window.scrollTo(0,0);') #return to top of page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to the Website\n",
    "Use the webdriver to locate the website and login with user credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=input('what is your username?') #prompt user to input their username\n",
    "password=input('what is your password?') #prompt user to input their password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open LinkedIn with chrome webdriver and login\n",
    "#note that webdriver must be installed\n",
    "\n",
    "#initiate the webdriver\n",
    "options=webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors-spki-list')\n",
    "options.add_argument('--ignore-ssl-errors')\n",
    "driver=webdriver.Chrome(chrome_options=options)\n",
    "\n",
    "#login to linkedin\n",
    "driver.get('https://www.linkedin.com/') #navigate to the page\n",
    "username=driver.find_element_by_id(\"login-email\") #username form field\n",
    "password=driver.find_element_by_id(\"login-password\") #password form field\n",
    "username.send_keys(username_input) #send username string to username field\n",
    "password.send_keys(password_input) #send pw to pw field\n",
    "driver.find_element_by_id(\"login-submit\").click() #locate and click submit button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=input('What city/region do you want to work in?')\n",
    "\n",
    "driver.get('https://www.linkedin.com/jobs/search/?location='+city) #go to search results for your city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Relevant Companies\n",
    "After spending time on Glassdoor and looking around the web for other credible sources of company data by region, I considered that LinkedIn itself would provide a good source of companies by looking at jobs by region, then scraping the posting company for those job ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_btn=driver.find_element_by_xpath('//*[@class=\"jobs-search-dropdown__trigger-icon\"]')\n",
    "split_btn.click()\n",
    "try:\n",
    "    classic_btn=driver.find_element_by_xpath('//*[@class=\"jobs-search-dropdown__option-button jobs-search-dropdown__option-button--single \"]')\n",
    "    classic_btn.click()\n",
    "except:\n",
    "    print('already in classic mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_end=False #initiate results_end as False\n",
    "companies_list=[] #create empty companies list container\n",
    "p=0 #initiate page at 0\n",
    "while results_end==False:\n",
    "    p=p+1\n",
    "    print('starting page='+str(p))\n",
    "    scrolljs\n",
    "    companies=driver.find_elements_by_xpath('//h4[@class=\"job-card-search__company-name\"]') #find all companies on page\n",
    "\n",
    "    #harvest all companies on page\n",
    "    for c in companies:\n",
    "        companies_list.append(c.text)\n",
    "\n",
    "    #click 'next' results\n",
    "    try: #see if the 'Next' link is found on the current page\n",
    "        nextpg_btn=driver.find_element_by_xpath('//*[@class=\"artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"]')\n",
    "        nextpg_btn.click()\n",
    "        time.sleep(random.randint(2,10))\n",
    "    except: #if the 'Next' link can't be found just change results_end to False and continue to next while loop iteration\n",
    "        results_end=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduplicate and save scraped companies to a csv file\n",
    "companies_list=list(set(companies_list)) #deduplicate the list\n",
    "companies_list=[c for c in companies_list if c!=''] #get rid of the '' companies\n",
    "pd.DataFrame({'company':companies_list}).to_csv('companies.csv',index=False, header=False) #export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through Companies to Find People\n",
    "For each company:\n",
    "1. Find Relevant People in the Company (using search terms for tagline/title)\n",
    "2. View their Profile if they are in your city of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get search terms for the scrape\n",
    "df=pd.read_csv('search_terms.csv',header=None) #read csv\n",
    "search_terms=df.iloc[:,0].tolist() #convert to list\n",
    "\n",
    "#get companies to scrape\n",
    "df=pd.read_csv('companies.csv',header=None) #read csv\n",
    "companies=df.iloc[:,0].tolist() #convert to list\n",
    "\n",
    "#create a repository dataframe to which all scraped data will be written\n",
    "repo=pd.DataFrame()\n",
    "\n",
    "#Loop through companies in list (LOOP #1.0)\n",
    "for company in companies:\n",
    "\n",
    "    url_company_search='https://www.linkedin.com/search/results/companies/?keywords='+company+'&origin=SWITCH_SEARCH_VERTICAL'\n",
    "    driver.get(url_company_search) #navigate to the search results page\n",
    "\n",
    "    #locate first element in search results\n",
    "    link=driver.find_element_by_xpath(\"//div[@class='blended-srp-results-js pt0 pb4 ph0 container-with-shadow']/ul/li[1]/div/div/div[2]/a\")\n",
    "    company_link=link.get_attribute(\"href\") #get the target of the link\n",
    "\n",
    "    #store name and industry of the company \n",
    "    company_name=driver.find_element_by_xpath(\"//div[@class='blended-srp-results-js pt0 pb4 ph0 container-with-shadow']/ul/li[1]/div/div/div[2]/a/h3\").text\n",
    "    try:\n",
    "        company_industry=driver.find_element_by_xpath(\"//div[@class='blended-srp-results-js pt0 pb4 ph0 container-with-shadow']/ul/li[1]/div/div/div[2]/p\").text\n",
    "    except:\n",
    "        company_industry='Unknown'\n",
    "    driver.get(company_link) #browse to the company link's target\n",
    "\n",
    "    #locate element with link to go to employees of the company\n",
    "    link=driver.find_element_by_xpath('//span[@class=\"org-company-employees-snackbar__see-all-employees-link\"]/a')\n",
    "    employees_link=link.get_attribute(\"href\") #get target url of emps link\n",
    "    driver.get(employees_link) #go to the link's destination url\n",
    "\n",
    "    #store text of the element string that shows # of employees globally\n",
    "    employees_num=driver.find_element_by_xpath('//h3[@class=\"search-results__total t-14 t-black--light t-normal pl5 pt4 clear-both\"]').text\n",
    "    employees_num2=re.findall('[0-9,]+',employees_num)\n",
    "    employees_num2=re.sub('[,]','',employees_num2[0])\n",
    "    employees_link_uk=employees_link+'&facetGeoRegion=%5B\"gb%3A0\"%5D' #get url of UK employees\n",
    "    driver.get(employees_link_uk) #browse to uk employees page\n",
    "\n",
    "    #store text of the element string that shows # of employees in UK\n",
    "    try:\n",
    "        employees_num_uk=driver.find_element_by_xpath('//h3[@class=\"search-results__total t-14 t-black--light t-normal pl5 pt4 clear-both\"]').text\n",
    "    except:\n",
    "        employees_num_uk='0'\n",
    "    employees_num_uk2=re.findall('[0-9,]+',employees_num_uk)\n",
    "    employees_num_uk2=re.sub('[,]','',employees_num_uk2[0])\n",
    "\n",
    "    #print company's linkedin profile details to window\n",
    "    print('company name='+company_name+\\\n",
    "          '\\nindustry='+company_industry+\\\n",
    "          '\\nlink='+company_link+\\\n",
    "          '\\nglobal employees='+employees_num2+\\\n",
    "          '\\nuk employees='+employees_num_uk2)\n",
    "    \n",
    "    \n",
    "    #Loop through all search result pages and scrape the name, title, and location (LOOP #2.0)\n",
    "    for t in search_terms:\n",
    "        \n",
    "        results_url=employees_link_uk+'&title='+t\n",
    "        driver.get(results_url)\n",
    "        shutdown=False\n",
    "        i=0\n",
    "        #Loop through all result pages for this search iteration (LOOP #3.0)\n",
    "        while shutdown==False:\n",
    "\n",
    "            #store current url for later use\n",
    "            url_current=driver.current_url\n",
    "\n",
    "            #find all employees in page\n",
    "            scrolljs()\n",
    "            try:\n",
    "                #find names on the page (or LinkedIn Member which has a different class as shown below)\n",
    "                emps=driver.find_elements_by_xpath('//span[@class=\"name actor-name\"]|//span[@class=\"actor-name\"]')\n",
    "            except:\n",
    "                #prepare for next iteration or stop if no more result pages!\n",
    "                try:\n",
    "                    driver.find_element_by_xpath('//button[@class=\"artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"]').click()\n",
    "                    time.sleep(random.randint(4,15))\n",
    "                    i=i+1\n",
    "                except:\n",
    "                    shutdown=True\n",
    "\n",
    "                continue\n",
    "                \n",
    "            emps_list=[]\n",
    "\n",
    "            for emp in emps: #loop #3.1\n",
    "                emps_list.append(emp.text.strip())\n",
    "            emps_list\n",
    "\n",
    "            #find all taglines/titles in url string\n",
    "            jtitles=driver.find_elements_by_xpath('//p[@class=\"subline-level-1 t-14 t-black t-normal search-result__truncate\"]')\n",
    "            jtitles_list=[]\n",
    "\n",
    "            for jtitle in jtitles: #loop #3.2\n",
    "                jtitles_list.append(jtitle.text.strip())\n",
    "            jtitles_list\n",
    "\n",
    "            #find all locations in url string\n",
    "            locations=driver.find_elements_by_xpath('//p[@class=\"subline-level-2 t-12 t-black--light t-normal search-result__truncate\"]')\n",
    "            locations_list=[]\n",
    "            for location in locations: #loop #3.3\n",
    "                locations_list.append(location.text.strip())\n",
    "            locations_list\n",
    "\n",
    "            #find all employee url links in page\n",
    "            plinks=driver.find_elements_by_xpath('//div[@class=\"search-result__info pt3 pb4 ph0\"]/a')\n",
    "            urls_list=[]\n",
    "\n",
    "            for plink in plinks: #loop #3.4\n",
    "                url=plink.get_attribute('href')\n",
    "                urls_list.append(url)\n",
    "            urls_list\n",
    "            \n",
    "            j=0\n",
    "            for url in urls_list: #loop #3.5\n",
    "                #view url/profile if in city of interest\n",
    "                if city in locations_list[j]:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(random.randint(5,15))\n",
    "                j=j+1\n",
    "\n",
    "            #combine these lists into a dataframe\n",
    "            pgdata=pd.DataFrame({'company':company,\n",
    "                               'employee':emps_list,\n",
    "                               'job_tagline':jtitles_list,\n",
    "                               'location':locations_list,\n",
    "                               'profile_url':urls_list})\n",
    "            repo=repo.append(pgdata)\n",
    "\n",
    "            #print info to log\n",
    "            print('criteria=(\"'+t+'\")'+\\\n",
    "                  ' iteration='+str(i)+' '+\\\n",
    "                  str(time.strftime('%H:%M:%S'))+\\\n",
    "                  ' rows added='+str(len(pgdata))+\\\n",
    "                  ' repo.shape='+str(repo.shape))\n",
    "\n",
    "            #return to parent results page\n",
    "            driver.get(url_current)\n",
    "            scrolljs()\n",
    "\n",
    "            #prepare for next iteration or stop if no more result pages!\n",
    "            try:\n",
    "                driver.find_element_by_xpath('//button[@class=\"artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"]').click()\n",
    "                time.sleep(random.randint(4,15))\n",
    "                i=i+1\n",
    "            except:\n",
    "                shutdown=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up and Write Log to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quit driver\n",
    "driver.quit()\n",
    "\n",
    "#get country from location field\n",
    "repo['country']=repo.location.str.extract('([\\w ]+)$').str.strip()\n",
    "#deduplicate results\n",
    "repo.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "#output to excel (with urls click-ready)\n",
    "#create directory for outputs if doesn't exist\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "writer=pd.ExcelWriter('outputs\\\\'+str(time.strftime('%d%b%y_%H%M'))+'_leadprospects.xlsx',\n",
    "                      engine='xlsxwriter',\n",
    "                      options={'strings_to_urls':True})\n",
    "repo.to_excel(writer,\n",
    "              sheet_name=company,\n",
    "              columns=['company','employee','job_tagline','location','country',\n",
    "                       'profile_url'],\n",
    "              index=False)\n",
    "writer.save()\n",
    "print('To see the results, check the file that has just been saved to:\\\n",
    "      outputs\\\\'+str(time.strftime('%d%b%y_%H%M'))+'_leadprospects.xlsx')\n",
    "input('click Enter to close this window')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
